import torch

class Config:

    def __init__(self):
        self.root_path = '../data/mit-bih-svt-s/'
        self.checkpoints = './checkpoints/'
        self.fs = None
        self.enc_in = None
        self.downsample_factor = 1
        self.actual_fs = None
        self.num_class = None
        self.c_out = None
        self.sample_selection_mode = 'by_disease'
        self.sample_count_limits = {'min_samples': 200, 'max_samples': 20000}
        self.selected_labels = ['N', 'V', 'S']
        self.label_to_type = None
        self.disease_mapping = {'N': ['N', 'L', 'R', 'e', 'j'], 'S': ['A', 'a', 'J', 'S', 'n', 'x'], 'V': ['V', 'F', 'f', 'r', '!'], 'F': ['F', 'f'], 'Q': ['Q', '/', 'E', '[', ']', '+', '~', '"', '=', 'B', '|', 'p', 'u', '^', 't', 'T', '*', 'D', '?']}
        self.disease_categories = {'N': 0, 'S': 1, 'V': 2, 'F': 3, 'Q': 4}
        self.disease_category_names = {v: k for k, v in self.disease_categories.items()}
        self.test_ratio = 0.15
        self.val_ratio = 0.15
        self.train_ratio = 0.7
        self.seed = 3
        self.batch_size = 256
        self.num_workers = 0
        self.mean_rr = None
        self.std_rr = None
        self.adaptive_patch_len = True
        self.power_of_two_patch = True
        self.adaptive_min_patch = 32
        self.adaptive_max_patch = 256
        self.default_patch_len = 128
        self.patch_len = None
        self.stride = None
        self.padding = 0
        self.num_consecutive_patches = 10
        self.num_patch = None
        self.preprocess_use_notch = False
        self.preprocess_notch_freq = 60.0
        self.preprocess_notch_q = 30.0
        self.preprocess_use_highpass = False
        self.preprocess_highpass_cutoff = 0.5
        self.preprocess_highpass_order = 2
        self.preprocess_use_wavelet = True
        self.preprocess_wavelet_name = 'bior3.7'
        self.preprocess_wavelet_level = 3
        self.preprocess_wavelet_threshold_scale = 0.7
        self.preprocess_use_median = True
        self.preprocess_median_kernel_ms = 50
        self.normalization_method = 'minmax'
        self.use_augmentation = False
        self.augment_prob = 0.5
        self.augment_minority_only = False
        self.augment_noise_types = ['gaussian', 'salt_pepper']
        self.augment_noise_std = 0.01
        self.augment_noise_amount = 0.005
        self.augment_adjust_position_prob = 0.1
        self.use_smote = False
        self.smote_k_neighbors = 5
        self.smote_type = 'regular'
        self.smote_sampling_strategy = 'auto'
        self.d_model = 512
        self.n_heads = 4
        self.e_layers = 3
        self.d_ff = 2048
        self.dropout = 0.2
        self.activation = 'gelu'
        self.output_attention = True
        self.attention_type = 'full'
        self.factor = 5
        self.d_keys = None
        self.d_values = None
        self.use_rotary_pos_enc = True
        self.use_conv_layers_in_encoder = False
        self.use_fan_layer = True
        self.fan_p_ratio = 0.25
        self.fan_activation = 'gelu'
        self.fan_with_gate = False
        self.positional_encoding_type = 'absolute'
        self.max_seq_len = 5000
        self.use_channel_attention = True
        self.use_cross_channel = False
        self.use_gat = True
        self.gat_heads = 2
        self.gat_dropout = 0.1
        self.gat_leaky_relu_negative_slope = 0.2
        self.use_combined_encoder = True
        self.use_morphology_cnn = True
        self.morphology_cnn_output_dim = 128
        self.morphology_cnn_kernels = [3, 5, 7, 9]
        self.morphology_cnn_num_filters = 32
        self.feature_fusion_strategy = 'concat'
        self.dynamic_feature_dim = 12
        self.pooling_mode = 'mean'
        self.task_name = 'classification'
        self.num_epochs = 1
        self.patience = 1
        self.save_epochs = [5, 10, 15, 20]
        self.learning_rate = 5e-06
        self.optimizer_type = 'AdamW'
        self.scheduler_type = 'CosineAnnealingLR'
        self.scheduler_step_size = 30
        self.scheduler_gamma = 0.1
        self.scheduler_T_max = self.num_epochs
        self.scheduler_eta_min = 1e-06
        self.scheduler_factor = 0.5
        self.scheduler_patience = 10
        self.scheduler_min_lr = 1e-07
        self.use_warmup = True
        self.warmup_epochs = 5
        self.warmup_start_lr = 1e-07
        self.use_two_stage_classifier = True
        self.stage1_loss_type = 'BCEWithLogits'
        self.stage1_loss_weight = 1.0
        self.stage1_pos_weight_strategy = 'inverse_frequency'
        self.stage1_focal_alpha = 0.5
        self.stage1_focal_gamma = 2.0
        self.stage1_triplet_margin = 1.0
        self.stage2_loss_type = 'CrossEntropy'
        self.stage2_loss_weight = 1.0
        self.num_abnormal_classes = None
        self.stage2_pos_weight_strategy = 'none'
        self.stage2_focal_alpha = 0.25
        self.stage2_focal_gamma = 2.0
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.use_confidence_rejection = True
        self.fixed_confidence_threshold = None
        self.max_rejection_rate = 0.3

    def update_calculated_params(self):
        if self.fs and self.downsample_factor > 0:
            self.actual_fs = self.fs / self.downsample_factor
        else:
            self.actual_fs = self.fs
        if self.num_class:
            self.c_out = self.num_class
        if self.scheduler_type == 'CosineAnnealingLR':
            self.scheduler_T_max = self.num_epochs
